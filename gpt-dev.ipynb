{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "627a78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘input.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b251417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read it to inspect it\n",
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563ebec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the number of chracters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the number of chracters: \" + str(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a2aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7562e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Unique characters of the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc1e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7997f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Enconder: take a string, output the list of integers assigned to each.\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# Decoder: Take a list  of integers, output a string\n",
    "decode = lambda s: [itos[c] for c in s] \n",
    "\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1182671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/williehernandez/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/williehernandez/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/williehernandez/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/63/07kl2cbn75v_2nlh0mv6bx_80000gn/T/ipykernel_984/97163526.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/williehernandez/Documents/github/buildgpt_from_zero/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Transform it into a torch of integers to be used in Deep Learning\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de data into training and evaluation\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164a00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 #Basically the batches for training fo the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a2552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Intuition: We want that our algorithm sees a sequence of integers from 1 to 8 (block size) and has the abillity to predict the next character.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")\n",
    "\n",
    "# Note: The transformer should be comfortable being context of size 1 and of size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2fbb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple batches at the same time to make use of the GPU\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of independent sequences will we process in parallel.\n",
    "block_size = 8 # maximum number of chracters in the same block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5e55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a batch of input and target sequences for training or validation.\n",
    "\n",
    "    This function randomly samples `batch_size` starting positions from either \n",
    "    the training or validation dataset and returns two tensors:\n",
    "      - x (inputs): sequences of length `block_size`\n",
    "      - y (targets): the same sequences shifted one position to the right\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : str\n",
    "        The dataset to draw from. Must be either:\n",
    "          - \"train\" : use the global variable `train_data`\n",
    "          - \"val\"   : use the global variable `val_data`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : torch.Tensor, shape (batch_size, block_size), dtype=torch.long\n",
    "        The input sequences, each of length `block_size`. \n",
    "        Each row corresponds to one sequence sampled from the dataset.\n",
    "\n",
    "    y : torch.Tensor, shape (batch_size, block_size), dtype=torch.long\n",
    "        The target sequences, each also of length `block_size`.\n",
    "        For each row, `y` is the same as `x` but shifted by one character/token.\n",
    "        (If x is [c1, c2, c3, ...], then y is [c2, c3, c4, ...])\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - `ix` contains the random starting indices for each sequence.\n",
    "    - The slicing ensures that `y` is the \"next-token prediction target\" \n",
    "      for `x`, which is the standard setup for language modeling.\n",
    "    - Both x and y are stacked into tensors so they can be processed in \n",
    "      parallel by the model (GPU-friendly).\n",
    "    \"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9930af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb311df",
   "metadata": {},
   "source": [
    "**B (Batch size)** = number of independent sequences processed in parallel.\n",
    "\n",
    "**T (Time / Context length)** = max number of tokens per sequence.\n",
    "\n",
    "**C (Channels / Classes)** = vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd738161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 65])\n",
      "Loss: 4.878634929656982\n",
      "Perplexity: 131.4510955810547\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BigramLanguageModel\n",
    "\n",
    "    This simple model learns to predict the next token using *only the current token*\n",
    "    (no context beyond one step = \"bigram\").\n",
    "\n",
    "    Tensor dimensions:\n",
    "        B = batch size\n",
    "            → number of independent sequences processed in parallel.\n",
    "\n",
    "        T = time (context length)\n",
    "            → maximum number of tokens per sequence.\n",
    "              Each row has T tokens, so total tokens in a batch = B * T.\n",
    "        C = vocab size\n",
    "            → number of output classes (the logits for each next token).\n",
    "\n",
    "    Flow:\n",
    "        idx: (B, T) integers [0, vocab_size-1]\n",
    "        targets: (B, T)\n",
    "        logits: (B, T, C) → reshaped to (B*T, C)\n",
    "        loss: scalar cross-entropy over all B*T predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding table: each input token maps to a row of length vocab_size.\n",
    "        # Shape: (vocab_size, vocab_size)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor\n",
    "            Shape (B, T) with token ids.\n",
    "        targets : torch.Tensor\n",
    "            Shape (B, T) with ground-truth next token ids.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Shape (B*T, C). Logits for every token position.\n",
    "        loss : torch.Tensor\n",
    "            Scalar mean cross-entropy loss.\n",
    "        \"\"\"\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # (B, T, C): each token index is mapped to a vocab-sized vector\n",
    "            logits_3d = self.token_embedding_table(inputs)\n",
    "\n",
    "            B, T, C = logits_3d.shape\n",
    "            # Flatten batch and time so CE sees (N, C) with N=B*T\n",
    "            logits = logits_3d.view(B * T, C)\n",
    "            flat_targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, flat_targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Autoregressive text generation.\n",
    "\n",
    "        Starting from an initial sequence of token indices, repeatedly sample\n",
    "        the next token using the model’s bigram predictions and append it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.Tensor\n",
    "            Shape (B, T) with initial token ids (the \"prompt\").\n",
    "        max_new_tokens : int\n",
    "            Number of tokens to generate beyond the initial prompt.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx : torch.Tensor\n",
    "            Shape (B, T + max_new_tokens). The original prompt followed by\n",
    "            the newly generated tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass: (B, T, C) logits for all positions\n",
    "            logits_3d = self.token_embedding_table(idx)\n",
    "\n",
    "            # Focus only on the last time step\n",
    "            # Shape: (B, C)\n",
    "            logits_last = logits_3d[:, -1, :]\n",
    "\n",
    "            # Convert logits to probabilities via softmax\n",
    "            probs = F.softmax(logits_last, dim=-1)  # (B, C)\n",
    "\n",
    "            # Sample next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # Append new token to the sequence along the time dimension\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# Perplexity = exp(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)     # (B*T, C) → (32, 65)\n",
    "print(\"Loss:\", loss.item())              # scalar\n",
    "print(\"Perplexity:\", perplexity.item())  # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa364873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(''.join(decode(m.generate(idx, max_new_tokens=100)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5bbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a1ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.404492139816284\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100000):\n",
    "\n",
    "  # Sample a batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # evaluate the loss\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2142bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bere ak hongacl GELusey wssisean thi's.\n",
      "When, w t med yfthon oklichay ol, I INI' dede blais cicen m \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(''.join(decode(m.generate(idx, max_new_tokens=100)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1542edb",
   "metadata": {},
   "source": [
    "# Mathematical simple option of Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a387df85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # Batch, Tokens, Vocab Size\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d69c1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b, i]\n",
    "# The attention here is that what is the average of the same vocab character for\n",
    "# all the precedent characters.\n",
    "xbow = torch.zeros(B, T, C, device=x.device, dtype=x.dtype)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow[b, t] = x[b, :t+1].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9689011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order not to repeat using for, using Matrix multiplication\n",
    "wei = torch.tril(torch.ones(T, T, device=x.device, dtype=x.dtype))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)   # row-normalize\n",
    "xbow2 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0120c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0] # They are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acfde20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key   = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)                  # (B, T, H)\n",
    "q = query(x)                # (B, T, H)\n",
    "v = value(x)                # (B, T, H)\n",
    "\n",
    "# scaled dot-product attention scores\n",
    "wei = q @ k.transpose(-2, -1) / (head_size ** 0.5)  # (B, T, T)\n",
    "\n",
    "# causal mask (lower triangular)\n",
    "tril = torch.tril(torch.ones(T, T, device=wei.device)).bool()\n",
    "wei = wei.masked_fill(~tril, float('-inf'))\n",
    "\n",
    "# normalize to probabilities\n",
    "wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "\n",
    "# weighted sum of values\n",
    "out = wei @ v  # (B, T, H)\n",
    "\n",
    "out.shape  # -> torch.Size([4, 8, 16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e7819",
   "metadata": {},
   "source": [
    "# Lessons from above\n",
    "\n",
    "- **Self-attention:** can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other (each batch works independently from other)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "359d60a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7+7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f3c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buildgpt_from_zero (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
