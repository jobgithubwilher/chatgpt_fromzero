{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de08de2",
   "metadata": {},
   "source": [
    "# Understand Chat-GPT (and self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0fae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db600b9",
   "metadata": {},
   "source": [
    "## 1. Get some playful data\n",
    "I will use the tinyshakespare, which is basically everything written by shakespare to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627a78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘input.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b251417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read it to inspect it\n",
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563ebec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the number of chracters: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the number of chracters: \" + str(len(text)))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7c5ef",
   "metadata": {},
   "source": [
    "## 2. Pre-process data\n",
    "In this exercise, tokens will be each character (capital and lower letters will be treated differently, also punctuation signs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7562e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Unique characters of the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67f294",
   "metadata": {},
   "source": [
    "We will mapp each character (defined above) to a number and two dictionaries to go from character to number and number to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc1e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7997f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Enconder: take a string, output the list of integers assigned to each.\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# Decoder: Take a list  of integers, output a string\n",
    "decode = lambda s: [itos[c] for c in s] \n",
    "\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b2775",
   "metadata": {},
   "source": [
    "# 3. Define the problem as Deep-Learning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1182671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Transform it into a torch of integers to be used in Deep Learning\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982d002",
   "metadata": {},
   "source": [
    "**Note:** Let's keep in mind that each character(punctuation, white space or charcter is included here as a data point).\n",
    "\n",
    "As a good practice, we take out some validation data out to test things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de data into training and evaluation\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c57f2",
   "metadata": {},
   "source": [
    "**Block:** When a language model is being trained, each next token will be feed not just by the latest token but also by a set a n previous tokens, this n is defined as the batch size. If we are predicting the last character, it will take into account the last 7 tokens, if we are predicting the 5th token it will take into account the previus 4, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164a00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 #Basically the batches for training fo the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a2552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Intuition: We want that our algorithm sees a sequence of integers from 1 to 8 (block size) and has the abillity to predict the next character.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")\n",
    "\n",
    "# Note: The transformer should be comfortable being context of size 1 and of size 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77faa7",
   "metadata": {},
   "source": [
    "**Batch:** Batch will be the number of blocks that are going to be processed at the same time. We use GPU, better use those in other to optimize all together. Important to remember that between batches they are not communicating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2fbb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple batches at the same time to make use of the GPU\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of independent sequences will we process in parallel.\n",
    "block_size = 8 # maximum number of chracters in the same block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5e55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a batch of input and target sequences for training or validation.\n",
    "\n",
    "    This function randomly samples `batch_size` starting positions from either \n",
    "    the training or validation dataset and returns two tensors:\n",
    "      - x (inputs): sequences of length `block_size`\n",
    "      - y (targets): the same sequences shifted one position to the right\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : str\n",
    "        The dataset to draw from. Must be either:\n",
    "          - \"train\" : use the global variable `train_data`\n",
    "          - \"val\"   : use the global variable `val_data`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : torch.Tensor, shape (batch_size, block_size), dtype=torch.long\n",
    "        The input sequences, each of length `block_size`. \n",
    "        Each row corresponds to one sequence sampled from the dataset.\n",
    "\n",
    "    y : torch.Tensor, shape (batch_size, block_size), dtype=torch.long\n",
    "        The target sequences, each also of length `block_size`.\n",
    "        For each row, `y` is the same as `x` but shifted by one character/token.\n",
    "        (If x is [c1, c2, c3, ...], then y is [c2, c3, c4, ...])\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - `ix` contains the random starting indices for each sequence.\n",
    "    - The slicing ensures that `y` is the \"next-token prediction target\" \n",
    "      for `x`, which is the standard setup for language modeling.\n",
    "    - Both x and y are stacked into tensors so they can be processed in \n",
    "      parallel by the model (GPU-friendly).\n",
    "    \"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c409e",
   "metadata": {},
   "source": [
    "We are basically selecting randomly blocks and batchez out of the original text (already encoded in numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9930af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c49803",
   "metadata": {},
   "source": [
    "## 4. Bigram Language Model\n",
    "What if we create a language model that predicts the next token just using the previous token (the easiest possible model just for learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb311df",
   "metadata": {},
   "source": [
    "**B (Batch size)** = number of independent sequences processed in parallel.\n",
    "\n",
    "**T (Time / Context length)** = max number of tokens per sequence.\n",
    "\n",
    "**C (Channels / Classes)** = vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd738161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([256, 65])\n",
      "Loss: 4.756565093994141\n",
      "Perplexity: 116.3456039428711\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BigramLanguageModel\n",
    "\n",
    "    This simple model learns to predict the next token using *only the current token*\n",
    "    (no context beyond one step = \"bigram\").\n",
    "\n",
    "    Tensor dimensions:\n",
    "        B = batch size\n",
    "            → number of independent sequences processed in parallel.\n",
    "\n",
    "        T = time (context length)\n",
    "            → maximum number of tokens per sequence.\n",
    "              Each row has T tokens, so total tokens in a batch = B * T.\n",
    "        C = vocab size\n",
    "            → number of output classes (the logits for each next token).\n",
    "\n",
    "    Flow:\n",
    "        idx: (B, T) integers [0, vocab_size-1]\n",
    "        targets: (B, T)\n",
    "        logits: (B, T, C) → reshaped to (B*T, C)\n",
    "        loss: scalar cross-entropy over all B*T predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding table: each input token maps to a row of length vocab_size.\n",
    "        # Shape: (vocab_size, vocab_size)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor\n",
    "            Shape (B, T) with token ids.\n",
    "        targets : torch.Tensor\n",
    "            Shape (B, T) with ground-truth next token ids.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Shape (B*T, C). Logits for every token position.\n",
    "        loss : torch.Tensor\n",
    "            Scalar mean cross-entropy loss.\n",
    "        \"\"\"\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # (B, T, C): each token index is mapped to a vocab-sized vector\n",
    "            logits_3d = self.token_embedding_table(inputs)\n",
    "\n",
    "            B, T, C = logits_3d.shape\n",
    "            # Flatten batch and time so CE sees (N, C) with N=B*T\n",
    "            logits = logits_3d.view(B * T, C)\n",
    "            flat_targets = targets.view(B * T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, flat_targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Autoregressive text generation.\n",
    "\n",
    "        Starting from an initial sequence of token indices, repeatedly sample\n",
    "        the next token using the model’s bigram predictions and append it.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.Tensor\n",
    "            Shape (B, T) with initial token ids (the \"prompt\").\n",
    "        max_new_tokens : int\n",
    "            Number of tokens to generate beyond the initial prompt.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx : torch.Tensor\n",
    "            Shape (B, T + max_new_tokens). The original prompt followed by\n",
    "            the newly generated tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass: (B, T, C) logits for all positions\n",
    "            logits_3d = self.token_embedding_table(idx)\n",
    "\n",
    "            # Focus only on the last time step\n",
    "            # Shape: (B, C)\n",
    "            logits_last = logits_3d[:, -1, :]\n",
    "\n",
    "            # Convert logits to probabilities via softmax\n",
    "            probs = F.softmax(logits_last, dim=-1)  # (B, C)\n",
    "\n",
    "            # Sample next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # Append new token to the sequence along the time dimension\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# Perplexity = exp(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)     # (B*T, C) → (32, 65)\n",
    "print(\"Loss:\", loss.item())              # scalar\n",
    "print(\"Perplexity:\", perplexity.item())  # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1965d5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(''.join(decode(m.generate(idx, max_new_tokens=100)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebdba8",
   "metadata": {},
   "source": [
    "Above is a random text generated, it does not any human-readable as it was not optimized. It is just a sequence of random characters. The idea with the optimizer is reduce the perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a093815",
   "metadata": {},
   "source": [
    "**Intuition**\n",
    "\n",
    "When we say the vocabulary size is 65, this means there are 65 possible tokens that the model can generate at each step. The logits tensor has the shape B × T, where B represents the batch size and T is the number of tokens in the sequence. Each entry in this tensor corresponds to a score (before applying softmax) that represents how likely the model thinks each of the 65 vocabulary tokens is to be the next token. In other words, for every position in the sequence, the model produces a probability distribution over the entire vocabulary.\n",
    "\n",
    "**Perplexity:** is a way to measure how well a language model predicts text.\n",
    "\n",
    "If the model is very confident and usually correct, the perplexity is low.\n",
    "\n",
    "If the model is often unsure or wrong, the perplexity is high.\n",
    "\n",
    "You can think of perplexity as “How surprised is the model when it sees the real next word?”\n",
    "\n",
    "If perplexity = 1 → the model is perfect (always predicts the right word with probability 1).\n",
    "\n",
    "If perplexity = vocabulary size (e.g., 65) → the model is just guessing randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572140f",
   "metadata": {},
   "source": [
    "The perplexity is defined as $ \\text{Perplexity} = \\exp\\!\\left(-\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i \\mid w_{<i})\\right) $.\n",
    "\n",
    "$P(w_i \\mid w_{<i}) $. is the probability of the model assigining to the right token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49239d",
   "metadata": {},
   "source": [
    "### How about if we now, we optimize it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b5bbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bd3ea",
   "metadata": {},
   "source": [
    "After applying Adam optimization, our goal is to get a better perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28a1ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4275641441345215\n",
      "Loss: 2.4275641441345215\n",
      "Perplexity: 11.331247329711914\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100000):\n",
    "\n",
    "  # Sample a batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # evaluate the loss\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "# Perplexity = exp(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "print(\"Loss:\", loss.item())              # scalar\n",
    "print(\"Perplexity:\", perplexity.item())  # scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9ba2e",
   "metadata": {},
   "source": [
    "Perplexity was reduced to almost a 10%, which shows the ability of the optimizer.\n",
    "\n",
    "The below is what will happen if we just randomly create random text, how will it look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2142bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ar bungin u,\n",
      "Myo fe.\n",
      "Heabeayoisurbet beld, bybe s; bonsth, t hid Y:\n",
      "Plooweaureiss fou, Fouffltyorbra\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(''.join(decode(m.generate(idx, max_new_tokens=100)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1542edb",
   "metadata": {},
   "source": [
    "## 5. Mathematical simple option of Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a387df85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # Batch, Tokens, Vocab Size\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d6c75",
   "metadata": {},
   "source": [
    "x is a torch that represent the number of batches * the number of tokens * the size of the vocabulary (the probability of the next token being any of the vocab size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b5f15",
   "metadata": {},
   "source": [
    "**Intuition:** The easiest way to know the probability of the next token is just by averagin the probability of this next token of the previous tokens within the same block.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d69c1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b, i]\n",
    "# The attention here is that what is the average of the same vocab character for\n",
    "# all the precedent characters.\n",
    "xbow = torch.zeros(B, T, C, device=x.device, dtype=x.dtype)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow[b, t] = x[b, :t+1].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9689011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order not to repeat using for, using Matrix multiplication\n",
    "wei = torch.tril(torch.ones(T, T, device=x.device, dtype=x.dtype))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)   # row-normalize\n",
    "xbow2 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfde20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key   = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)                  # (B, T, H)\n",
    "q = query(x)                # (B, T, H)\n",
    "v = value(x)                # (B, T, H)\n",
    "\n",
    "# scaled dot-product attention scores\n",
    "wei = q @ k.transpose(-2, -1) / (head_size ** 0.5)  # (B, T, T)\n",
    "\n",
    "# causal mask (lower triangular)\n",
    "tril = torch.tril(torch.ones(T, T, device=wei.device)).bool()\n",
    "wei = wei.masked_fill(~tril, float('-inf'))\n",
    "\n",
    "# normalize to probabilities\n",
    "wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "\n",
    "# weighted sum of values\n",
    "out = wei @ v  # (B, T, H)\n",
    "\n",
    "out.shape  # -> torch.Size([4, 8, 16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250ff1a",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Self-attention** is a way for each word in a sequence to decide which other words matter to it when forming its meaning. In the code, every word is first turned into three vectors: **a query** (what it’s looking for), **a key** (what it contains), and **a value** (the information it can share). \n",
    "\n",
    "1. Each query compares itself with all keys using dot products to produce attention scores, which measure relevance.\n",
    "2. These scores are restricted, masked so words cannot look into the future\n",
    "3. Tokens pass through a softmax to become probabilities. This creates an “attention distribution” that tells a word how much to focus on each other word, including itself.\n",
    "4.  Finally, each word gathers the values of all words, weighted by these attention probabilities, to create a new, context-aware representation of itself. \n",
    "\n",
    "In other words, instead of treating words in isolation, self-attention lets them dynamically borrow information from others:\n",
    "\n",
    "*Classic Example:*  “bank” in “river bank” pays attention to “river,” while “bank” in “money bank” focuses on “money.”\n",
    "\n",
    "The output of the code is a matrix where every word has been enriched with the context it deems most important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee54a2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buildgpt_from_zero (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
