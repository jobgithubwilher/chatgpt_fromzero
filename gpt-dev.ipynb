{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627a78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-17 20:47:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-09-17 20:47:42 (9.40 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b251417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read it to inspect it\n",
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563ebec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the number of chracters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the number of chracters: \" + str(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a2aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7562e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Unique characters of the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc1e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7997f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Enconder: take a string, output the list of integers assigned to each.\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# Decoder: Take a list  of integers, output a string\n",
    "decode = lambda s: [itos[c] for c in s] \n",
    "\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Transform it into a torch of integers to be used in Deep Learning\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d4a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de data into training and evaluation\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "164a00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 #Basically the batches for training fo the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Intuition: We want that our algorithm sees a sequence of integers from 1 to 8 (block size) and has the abillity to predict the next character.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")\n",
    "\n",
    "# Note: The transformer should be comfortable being context of size 1 and of size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2fbb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple batches at the same time to make use of the GPU\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of independent sequences will we process in parallel.\n",
    "block_size = 8 # maximum number of chracters in the same block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5e55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a batch of input and target sequences for training or validation.\n",
    "\n",
    "    This function randomly samples `batch_size` starting positions from either \n",
    "    the training or validation dataset and returns two tensors:\n",
    "      - x (inputs): sequences of length `block_size`\n",
    "      - y (targets): the same sequences shifted one position to the right\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : str\n",
    "        The dataset to draw from. Must be either:\n",
    "          - \"train\" : use the global variable `train_data`\n",
    "          - \"val\"   : use the global variable `val_data`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : torch.Tensor, shape (batch_size, block_size), dtype=torch.long\n",
    "        The input sequences, each of length `block_size`. \n",
    "        Each row corresponds to one sequence sampled from the dataset.\n",
    "\n",
    "    y : torch.Tensor, shape (batch_size, block_size), dtype=torch.long\n",
    "        The target sequences, each also of length `block_size`.\n",
    "        For each row, `y` is the same as `x` but shifted by one character/token.\n",
    "        (If x is [c1, c2, c3, ...], then y is [c2, c3, c4, ...])\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - `ix` contains the random starting indices for each sequence.\n",
    "    - The slicing ensures that `y` is the \"next-token prediction target\" \n",
    "      for `x`, which is the standard setup for language modeling.\n",
    "    - Both x and y are stacked into tensors so they can be processed in \n",
    "      parallel by the model (GPU-friendly).\n",
    "    \"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9930af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb311df",
   "metadata": {},
   "source": [
    "**B (Batch size)** = number of independent sequences processed in parallel.\n",
    "\n",
    "**T (Time / Context length)** = max number of tokens per sequence.\n",
    "\n",
    "**C (Channels / Classes)** = vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd738161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 65])\n",
      "Loss: 4.5847039222717285\n",
      "Perplexity: 97.97417449951172\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BigramLanguageModel\n",
    "\n",
    "    This simple model learns to predict the next token using *only the current token*\n",
    "    (no context beyond one step = \"bigram\").\n",
    "\n",
    "    Tensor dimensions:\n",
    "        B = batch size\n",
    "            → number of independent sequences processed in parallel.\n",
    "           \n",
    "        T = time (context length)\n",
    "            → maximum number of tokens per sequence.\n",
    "              Each row has T tokens, so total tokens in a batch = B * T.\n",
    "        C = vocab size\n",
    "            → number of output classes (the logits for each next token).\n",
    "\n",
    "    Flow:\n",
    "        idx: (B, T) integers [0, vocab_size-1]\n",
    "        targets: (B, T)\n",
    "        logits: (B, T, C) → reshaped to (B*T, C)\n",
    "        loss: scalar cross-entropy over all B*T predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding table: each input token maps to a row of length vocab_size.\n",
    "        # Shape: (vocab_size, vocab_size)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor\n",
    "            Shape (B, T) with token ids.\n",
    "        targets : torch.Tensor\n",
    "            Shape (B, T) with ground-truth next token ids.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Shape (B*T, C). Logits for every token position.\n",
    "        loss : torch.Tensor\n",
    "            Scalar mean cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # (B, T, C): each token index is mapped to a vocab-sized vector\n",
    "        logits_3d = self.token_embedding_table(inputs)\n",
    "\n",
    "        B, T, C = logits_3d.shape\n",
    "        # Flatten batch and time so CE sees (N, C) with N=B*T\n",
    "        logits = logits_3d.view(B * T, C)\n",
    "        flat_targets = targets.view(B * T)\n",
    "\n",
    "        loss = F.cross_entropy(logits, flat_targets)\n",
    "        return logits, loss\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# Perplexity = exp(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)     # (B*T, C) → (32, 65)\n",
    "print(\"Loss:\", loss.item())              # scalar\n",
    "print(\"Perplexity:\", perplexity.item())  # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa364873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bbf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(65, 65)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1ea47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buildgpt_from_zero (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
